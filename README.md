üöÄ **Supervised Learning with Scikit-Learn - DataCamp** üöÄ

Este reposit√≥rio √© dedicado a anota√ß√µes e testes de c√≥digo do curso "Supervised Learning with Scikit-Learn" do DataCamp. Embora a maior parte do c√≥digo seja executada diretamente na plataforma do DataCamp, optei por testar alguns conceitos localmente para aprofundar meu entendimento.

üìä **O curso aborda**:
- **Cria√ß√£o de Modelos Preditivos**: Desenvolvimento de modelos para prever se um cliente ir√° deixar um servi√ßo, identificar se um indiv√≠duo tem diabetes e at√© classificar o g√™nero de uma m√∫sica. O curso tamb√©m cobre a cria√ß√£o de Pipelines utilizando o scikit-learn para normaliza√ß√£o de dados e transforma√ß√£o com imputadores.
- **Otimiza√ß√£o e Avalia√ß√£o**: O curso detalha a aplica√ß√£o de valida√ß√£o cruzada e o ajuste de par√¢metros dos modelos utilizando t√©cnicas de GridSearch e RandomSearch para fine-tuning.

**Discuss√µes Interessantes:**

- **Intui√ß√£o sobre o ajuste do par√¢metro `n_neighbors` no modelo k-nearest neighbors (KNN)**:

  O par√¢metro `n_neighbors` define o n√∫mero de vizinhos considerados ao classificar um novo ponto de dados. Um valor maior pode aumentar a complexidade do modelo e, enquanto pode melhorar a performance, tamb√©m pode levar ao overfitting. A imagem abaixo ilustra a acur√°cia para diferentes n√∫meros de vizinhos em um modelo de classifica√ß√£o treinado e testado com o dataset Iris do scikit-learn.

  ![KNN Accuracy](images/knn.png)

- **Uso do Mean Squared Error Negativo na Valida√ß√£o Cruzada**:

  Na valida√ß√£o cruzada, as m√©tricas de avalia√ß√£o geralmente s√£o otimizadas para maximiza√ß√£o, como a acur√°cia. No entanto, quando trabalhamos com m√©tricas que devem ser minimizadas, como o Mean Squared Error (MSE), √© necess√°rio ajustar a abordagem. Para isso, utilizamos o MSE negativo.

  O MSE √© uma m√©trica que queremos minimizar (quanto menor, melhor). No entanto, o padr√£o do `cross_val_score` √© maximizar a m√©trica fornecida. Portanto, ao usar o MSE, voc√™ deve fornecer o MSE negativo como a m√©trica para maximiza√ß√£o. Dessa forma, maximizar o MSE negativo √© equivalente a minimizar o MSE.

  O exemplo abaixo mostra como realizar a valida√ß√£o cruzada com MSE negativo:

  ```python
  from sklearn.linear_model import Ridge
  from sklearn.model_selection import cross_val_score
  import numpy as np

  # Cria√ß√£o de X e y
  X = music_dummies.drop('popularity', axis=1).values
  y = music_dummies['popularity'].values

  # Instancia√ß√£o do modelo Ridge
  ridge = Ridge(alpha=0.2)

  # Realiza√ß√£o da valida√ß√£o cruzada com MSE negativo
  scores = cross_val_score(ridge, X, y, cv=kf, scoring="neg_mean_squared_error")

  # C√°lculo do RMSE
  rmse = np.sqrt(-scores)
  print("RMSE M√©dio: {}".format(np.mean(rmse)))
  print("Desvio Padr√£o do Array de Target: {}".format(np.std(y)))
